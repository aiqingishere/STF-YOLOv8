import torch
import torch.nn as nn
import torch.nn.functional as F


class TextResidualFusion(nn.Module):
    """
    CLIP Text â†’ CNN Feature çš„æ®‹å·®è°ƒåˆ¶æ¨¡å—
    - ä¸æ”¹å˜ feature map å°ºå¯¸
    - channel-wise modulation
    """

    def __init__(self, text_dim: int, feat_dim: int):
        super().__init__()

        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, feat_dim),
            nn.SiLU(),
            nn.Linear(feat_dim, feat_dim)
        )

        # âš ï¸ å…³é”®ï¼šä¸è¦åˆå§‹åŒ–ä¸º 0
        self.scale = nn.Parameter(torch.tensor(0.1))  # æˆ– 0.05
                # â­â­â­ å…³é”®ï¼šè¡¥ strideï¼Œæ¬ºéª— YOLO çš„ç»Ÿä¸€é€»è¾‘
        self.stride = torch.tensor([1])

    def forward(self, feat: torch.Tensor, text_feat: torch.Tensor | None):
        """
        feat: [B, C, H, W]
        text_feat: [B, text_dim]

        """

        #raise RuntimeError("TEXT FUSION IS CALLED")
        print("ğŸ”¥ TEXT FUSION EXECUTED")


        
        # CLIP æ ‡å‡†æ“ä½œ
        text_feat = F.normalize(text_feat, dim=-1)

        # [B, C]
        t = self.text_proj(text_feat)

        # channel-wise gate
        gamma = torch.sigmoid(t).unsqueeze(-1).unsqueeze(-1)

        # residual modulation
        return feat * (1.0 + self.scale * gamma)


# å…¼å®¹æ—§å¼•ç”¨
TextFusion = TextResidualFusion
